{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 统计分布\n",
    "## 正态分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#概率密度函数\n",
    "#stats.norm.pdf()->x（数据）,loc（mu均值）,scale（sigma方差）\n",
    "\n",
    "#累积概率密度函数\n",
    "#stats.norm.cdf()->x（数据）,loc（mu均值）,scale（sigma方差）\n",
    "\n",
    "##### COMPUTATION（随机样本）#####\n",
    "# DECLARING THE \"TRUE\" PARAMETERS UNDERLYING THE SAMPLE\n",
    "mu_real = 10\n",
    "sigma_real = 2\n",
    "# DRAW A SAMPLE OF N=1000(从生成的正态数据中取1000个数据)\n",
    "np.random.seed(42)\n",
    "sample = stats.norm.rvs(loc=mu_real, scale=sigma_real, size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 均匀分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF\n",
    "stats.uniform.pdf(np.linspace(-4, 4, 100)) \n",
    "# CDF\n",
    "stats.uniform.cdf(np.linspace(-4, 4, 100))\n",
    "\n",
    "##### COMPUTATION（随机样本）#####\n",
    "# draw a single sample\n",
    "np.random.seed(42)\n",
    "# draw 10 samples\n",
    "binom.rvs(p=0.3, n=10, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二项式分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF->p概率,n个数\n",
    "stats.binom.pmf(np.arange(20), p=.5, n=20)\n",
    "# CDF->p概率,n个数\n",
    "stats.binom.cdf(np.arange(20), p=.5, n=20)\n",
    "\n",
    "##### COMPUTATION（随机样本）#####\n",
    "# draw a single sample\n",
    "np.random.seed(42)\n",
    "# draw 10 samples\n",
    "stats.binom.rvs(p=0.3, n=10, size=10)\n",
    "\n",
    "# 算概率\n",
    "from scipy.stats import binom\n",
    "# probability of x less or equal 0.3\n",
    "print(\"P(X <=3) = {}\".format(binom.cdf(k=3, p=0.3, n=10)))\n",
    "# probability of x in [-0.2, +0.2]\n",
    "print(\"P(2 < X <= 8) = {}\".format(binom.cdf(k=8, p=0.3, n=10) - binom.cdf(k=2, p=0.3, n=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beta分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF\n",
    "stats.beta.pdf(np.linspace(0, 1, 100),a=2,b=2)\n",
    "\n",
    "# CDF\n",
    "stats.beta.cdf(np.linspace(0, 1, 100),a=2,b=2)\n",
    "\n",
    "##### COMPUTATION（随机样本）#####\n",
    "# draw a single sample\n",
    "np.random.seed(42)\n",
    "# draw 10 samples\n",
    "stats.beta.rvs(a=2, b=2, size=10)\n",
    "\n",
    "# 算概率\n",
    "from scipy.stats import beta\n",
    "# probability of x less or equal 0.3\n",
    "print(\"P(X <0.3) = {:.3}\".format(beta.cdf(a=2, b=2, x=0.3)))\n",
    "# probability of x in [-0.2, +0.2]\n",
    "print(\"P(-0.2 < X < 0.2) = {:.3}\".format(beta.cdf(a=2, b=2, x=0.2) - beta.cdf(a=2, b=2, x=-0.2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卡方分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF->df自由度\n",
    "stats.chi2.pdf(np.linspace(0, 20, 100), df=4)\n",
    "\n",
    "# CDF->df自由度\n",
    "stats.chi2.cdf(np.linspace(0, 20, 100), df=4)\n",
    "\n",
    "##### COMPUTATION（随机样本）#####\n",
    "# draw a single sample\n",
    "np.random.seed(42)\n",
    "# draw 10 samples\n",
    "chi2.rvs(df=4, size=10)\n",
    "\n",
    "# 算概率\n",
    "from scipy.stats import chi2\n",
    "# probability of x less or equal 0.3\n",
    "print(\"P(X <=3) = {}\".format(chi2.cdf(x=3, df=4)))\n",
    "# probability of x in [-0.2, +0.2]\n",
    "print(\"P(2 < X <= 8) = {}\".format(chi2.cdf(x=8, df=4) - chi2.cdf(x=2, df=4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 泊松分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF\n",
    "stats.poisson.pmf(np.arange(20), mu=5)\n",
    "\n",
    "# CDF\n",
    "stats.poisson.cdf(np.arange(20), mu=5)\n",
    "\n",
    "##### COMPUTATION（随机样本） #####\n",
    "# DECLARING THE \"TRUE\" PARAMETERS UNDERLYING THE SAMPLE\n",
    "lambda_real = 7\n",
    "# DRAW A SAMPLE OF N=1000\n",
    "np.random.seed(42)\n",
    "sample = stats.poisson.rvs(mu=lambda_real, size=1000)\n",
    "\n",
    "# 算概率\n",
    "from scipy.stats import poisson\n",
    "# probability of x less or equal 0.3\n",
    "print(\"P(X <=3) = {}\".format(poisson.cdf(k=3, mu=5)))\n",
    "# probability of x in [-0.2, +0.2]\n",
    "print(\"P(2 < X <= 8) = {}\".format(poisson.cdf(k=8, mu=5) - poisson.cdf(k=2, mu=5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 假设检验\n",
    "## 正态检验的两种方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 数据\n",
    "\n",
    "# 方法一\n",
    "#Shapiro-Wilk Test: https://en.wikipedia.org/wiki/Shapiro%E2%80%93Wilk_test\n",
    "shapiro_test, shapiro_p = scipy.stats.shapiro(x)\n",
    "print(\"Shapiro-Wilk Stat:\",shapiro_test, \" Shapiro-Wilk p-Value:\", shapiro_p)\n",
    "\n",
    "# 方法二\n",
    "k2, p = scipy.stats.normaltest(observed_temperatures)\n",
    "print('p:',p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "CW_mu = 98.6\n",
    "stats.ttest_1samp(df['Temperature'], CW_mu, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 卡方检验"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "observed_frequencies = [blacks_not_called, whites_not_called, whites_called, blacks_called]\n",
    "expected_frequencies = [expected_not_called/2, expected_not_called/2, expected_called/2, expected_called/2]\n",
    "stats.chisquare(f_obs = observed_frequencies,\n",
    "                f_exp = expected_frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方差分析\n",
    "## 单因素方差分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 呷哺呷哺3个城市不同用户评分\n",
    "from scipy.stats import f_oneway  \n",
    "a = [10,9,9,8,8,7,7,8,8,9]        \n",
    "b = [10,8,9,8,7,7,7,8,9,9]  \n",
    "c = [9,9,8,8,8,7,6,9,8,9]  \n",
    "f,p = f_oneway(a,b,c)  \n",
    "print (f)  \n",
    "print (p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多因素方差分析\n",
    "符号意义：\n",
    "\n",
    "-  （~）隔离因变量和自变量 (左边因变量，右边自变量 )  \n",
    "-  （+）分隔各个自变量\n",
    "-  （:）表示两个自变量交互影响   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 呷哺呷哺2个因素：环境等级，食材等级\n",
    "from scipy import stats  \n",
    "import pandas as pd  \n",
    "import numpy as np  \n",
    "from statsmodels.formula.api import ols  \n",
    "from statsmodels.stats.anova import anova_lm  \n",
    "\n",
    "  \n",
    "environmental =  [5,5,5,5,5,4,4,4,4,4,3,3,3,3,3,2,2,2,2,2,1,1,1,1,1]       \n",
    "ingredients    = [5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1,5,4,3,2,1]    \n",
    "score      =     [5,5,4,3,2,5,4,4,3,2,4,4,3,3,2,4,3,2,2,2,3,3,3,2,1]  \n",
    "  \n",
    "data = {'E':environmental, 'I':ingredients, 'S':score}  \n",
    "df = pd.DataFrame(data)  \n",
    "\n",
    "# 按变量关系进行数据处理后再做多因素方差分析\n",
    "formula = 'S~E+I+E:I'                                                                            \n",
    "model = ols(formula,df).fit()                   \n",
    "results = anova_lm(model)                       \n",
    "print (results)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 相关分析\n",
    "## Pearson相关系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.stats.pearsonr(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 斯皮尔曼等级相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.stats.spearmanr(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 肯德尔和谐系数(Kendall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.kendalltau(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 因子分析\n",
    "## PCA降维（重在方差）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标准化\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "print (X_std)\n",
    "\n",
    "# 获取相关系数\n",
    "cov_mat = np.cov(X_std.T)\n",
    "# 获取特征值和特征向量\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "\n",
    "print('Eigenvectors \\n%s' %eig_vecs)\n",
    "print('\\nEigenvalues \\n%s' %eig_vals)\n",
    "\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# 算占比最高的值\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [(i / tot)*100 for i in sorted(eig_vals, reverse=True)]\n",
    "print (var_exp)\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "# 使用占比最高的值进行降维\n",
    "matrix_w = np.hstack((eig_pairs[0][1].reshape(4,1),\n",
    "                      eig_pairs[1][1].reshape(4,1)))\n",
    "Y = X_std.dot(matrix_w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA降维（重在分类）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理字母标签为数字\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X = df[['sepal length in cm','sepal width in cm','petal length in cm','petal width in cm']].values\n",
    "y = df['class label'].values\n",
    "\n",
    "enc = LabelEncoder()\n",
    "label_encoder = enc.fit(y)\n",
    "# 标签以1开始\n",
    "y = label_encoder.transform(y) + 1\n",
    "# 还原标签真实对应数据(y标签是从1开始的)\n",
    "# y=label_encoder.inverse_transform(y-1)\n",
    "\n",
    "# 求不同特征维度上的均值\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "\n",
    "mean_vectors = []\n",
    "for cl in range(1,4):\n",
    "    mean_vectors.append(np.mean(X[y==cl], axis=0))\n",
    "    print('Mean Vector class %s: %s\\n' %(cl, mean_vectors[cl-1]))\n",
    "\n",
    "# 计算两个 4×4 维矩阵：类内散布矩阵和类间散布矩阵\n",
    "S_W = np.zeros((4,4))\n",
    "for cl,mv in zip(range(1,4), mean_vectors):\n",
    "    class_sc_mat = np.zeros((4,4))                  # scatter matrix for every class\n",
    "    for row in X[y == cl]:\n",
    "        row, mv = row.reshape(4,1), mv.reshape(4,1) # make column vectors\n",
    "        class_sc_mat += (row-mv).dot((row-mv).T)\n",
    "    S_W += class_sc_mat                             # sum class scatter matrices\n",
    "print('within-class Scatter Matrix:\\n', S_W)\n",
    "\n",
    "overall_mean = np.mean(X, axis=0)\n",
    "S_B = np.zeros((4,4))\n",
    "for i,mean_vec in enumerate(mean_vectors):  \n",
    "    n = X[y==i+1,:].shape[0]\n",
    "    mean_vec = mean_vec.reshape(4,1) # make column vector\n",
    "    overall_mean = overall_mean.reshape(4,1) # make column vector\n",
    "    S_B += n * (mean_vec - overall_mean).dot((mean_vec - overall_mean).T)\n",
    "print('between-class Scatter Matrix:\\n', S_B)\n",
    "# 求特征值和特征向量\n",
    "eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
    "\n",
    "for i in range(len(eig_vals)):\n",
    "    eigvec_sc = eig_vecs[:,i].reshape(4,1)   \n",
    "    print('\\nEigenvector {}: \\n{}'.format(i+1, eigvec_sc.real))\n",
    "    print('Eigenvalue {:}: {:.2e}'.format(i+1, eig_vals[i].real))\n",
    "# 排序看占比选最合适的\n",
    "#Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True)\n",
    "\n",
    "# Visually confirm that the list is correctly sorted by decreasing eigenvalues\n",
    "\n",
    "print('Eigenvalues in decreasing order:\\n')\n",
    "for i in eig_pairs:\n",
    "    print(i[0])\n",
    "print('Variance explained:\\n')\n",
    "eigv_sum = sum(eig_vals)\n",
    "for i,j in enumerate(eig_pairs):\n",
    "    print('eigenvalue {0:}: {1:.2%}'.format(i+1, (j[0]/eigv_sum).real))\n",
    "\n",
    "# 根据占比，选择两维特征\n",
    "W = np.hstack((eig_pairs[0][1].reshape(4,1), eig_pairs[1][1].reshape(4,1)))\n",
    "print('Matrix W:\\n', W.real)\n",
    "\n",
    "#降维\n",
    "X_lda = X.dot(W)\n",
    "assert X_lda.shape == (150,2), \"The matrix is not 150x2 dimensional.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 回归分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "X = data.height\n",
    "y = data.weight\n",
    "# 将截局增加到矩阵方便运算\n",
    "X = sm.add_constant(X)\n",
    "# 最小二乘法，并fit进行拟和数据\n",
    "model = sm.OLS(y, X).fit()\n",
    "# 查看回归系数\n",
    "model.params\n",
    "# 查看全部结果\n",
    "model.summary()\n",
    "#拟合的估计值\n",
    "y_ = res.fittedvalues\n",
    "# 补充：对变量进行分类\n",
    "groups[20:40] = 1\n",
    "groups[40:] = 2\n",
    "dummy = sm.categorical(groups, drop=True)\n",
    "dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 聚类分析\n",
    "## 层次聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#距离计算的 还有树状图\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "#进行层次聚类\n",
    "mergings = linkage(samples, method='complete') # method也可以是single\n",
    "#树状图结果\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "dendrogram(mergings,\n",
    "           labels=varieties,\n",
    "           leaf_rotation=90,\n",
    "           leaf_font_size=6,\n",
    ")\n",
    "plt.show()\n",
    "#得到标签结果\n",
    "#maximum height自己指定\n",
    "from scipy.cluster.hierarchy import fcluster\n",
    "labels = fcluster(mergings, 6, criterion='distance')\n",
    "\n",
    "df = pd.DataFrame({'labels': labels, 'varieties': varieties})\n",
    "ct = pd.crosstab(df['labels'], df['varieties'])\n",
    "\n",
    "# 补充（归一化处理）\n",
    "from sklearn.preprocessing import normalize\n",
    "samples = normalize(scores_df.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#聚类操作\n",
    "df = pd.read_csv('./datasets/ch1ex1.csv')\n",
    "points = df.values\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters=3)\n",
    "model.fit(points)\n",
    "labels = model.predict(points)\n",
    "#聚类中心\n",
    "centroids = model.cluster_centers_\n",
    "centroids_x = centroids[:,0]\n",
    "centroids_y = centroids[:,1]\n",
    "\n",
    "# pipeline方式处理数据\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "scaler = StandardScaler()\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "pipeline = make_pipeline(scaler, kmeans)\n",
    "pipeline.fit(samples)\n",
    "\n",
    "labels = pipeline.predict(samples)\n",
    "df = pd.DataFrame({'labels': labels, 'species': species})\n",
    "ct = pd.crosstab(df['labels'], df['species'])\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 轮廓系数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "score = metrics.silhouette_score(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eps为半径，min_samples为最低密度\n",
    "from sklearn.cluster import DBSCAN\n",
    "db = DBSCAN(eps=10,min_samples=2).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sklearn得分比较结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "\n",
    "X = np.random.randint(0, 100, (10, 4))\n",
    "y = np.random.randint(0, 3, 10)\n",
    "y.sort()\n",
    "# 分割训练集、测试集\n",
    "# random_state确保每次随机分割得到相同的结果\n",
    "# \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3., random_state=7) \n",
    "svm_classifier = svm.SVC()\n",
    "svm_classifier.fit(X_train,y_train)\n",
    "# score对于分类模型是准确率，回归模型是r2。本例是有标签样本即有输入x和输出y，无标签样本就是只有x\n",
    "# score还是accuracy_score都是判定模型的准确度或r2的标准\n",
    "# 而调试模型时train_score和test_score要尽可能一致模型才相对更好，可以通过调整模型的参数达到最终结果\n",
    "# 若train_score明显高于test_score，说明模型过拟合\n",
    "# 增大数据量是最好的办法，避免过拟和\n",
    "# 如果数据没那么多单纯复制没用，可以复制之后再加上随机抖动(生成个随机数组，shape和x一样就可以，直接相加)\n",
    "train_score = svm_classifier.score(X_train,y_train)\n",
    "test_score = svm_classifier.score(X_test,y_test)\n",
    "# 另一种查看结果\n",
    "from sklearn.metrics import accuracy_score\n",
    "#print '预测标签：', y_pred\n",
    "#print '真实标签：', y_true\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "#模型训练的问题不大后可以直接使用predict进行预测\n",
    "#保存模型\n",
    "import pickle\n",
    "with open('svm_model.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_classifier, f)\n",
    "#读取模型\n",
    "import numpy as np\n",
    "# 重新加载模型进行预测\n",
    "with open('svm_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "random_samples_index = np.random.randint(0, 1796, 5)\n",
    "random_samples = digits.data[random_samples_index, :]\n",
    "random_targets = digits.target[random_samples_index]\n",
    "random_predict = model.predict(random_samples)\n",
    "print(random_predict)\n",
    "print(random_targets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
